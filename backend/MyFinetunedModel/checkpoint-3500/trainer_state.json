{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9687201781359347,
  "eval_steps": 500,
  "global_step": 3500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.016965327112713393,
      "grad_norm": 0.24093033373355865,
      "learning_rate": 0.00019892564320045237,
      "loss": 4.4385,
      "step": 20
    },
    {
      "epoch": 0.033930654225426786,
      "grad_norm": 0.17502310872077942,
      "learning_rate": 0.0001977947413061917,
      "loss": 4.6901,
      "step": 40
    },
    {
      "epoch": 0.050895981338140175,
      "grad_norm": 0.6178982853889465,
      "learning_rate": 0.00019666383941193102,
      "loss": 4.3949,
      "step": 60
    },
    {
      "epoch": 0.06786130845085357,
      "grad_norm": 0.19208714365959167,
      "learning_rate": 0.00019553293751767035,
      "loss": 4.0009,
      "step": 80
    },
    {
      "epoch": 0.08482663556356695,
      "grad_norm": 0.4092497229576111,
      "learning_rate": 0.00019440203562340967,
      "loss": 3.2977,
      "step": 100
    },
    {
      "epoch": 0.10179196267628035,
      "grad_norm": 0.21576175093650818,
      "learning_rate": 0.000193271133729149,
      "loss": 2.8452,
      "step": 120
    },
    {
      "epoch": 0.11875728978899375,
      "grad_norm": 0.17212824523448944,
      "learning_rate": 0.00019214023183488835,
      "loss": 2.6343,
      "step": 140
    },
    {
      "epoch": 0.13572261690170714,
      "grad_norm": 0.1508650928735733,
      "learning_rate": 0.00019100932994062765,
      "loss": 2.5352,
      "step": 160
    },
    {
      "epoch": 0.15268794401442054,
      "grad_norm": 0.13115911185741425,
      "learning_rate": 0.000189878428046367,
      "loss": 2.5154,
      "step": 180
    },
    {
      "epoch": 0.1696532711271339,
      "grad_norm": 0.1694781333208084,
      "learning_rate": 0.0001887475261521063,
      "loss": 2.5316,
      "step": 200
    },
    {
      "epoch": 0.1866185982398473,
      "grad_norm": 0.15876241028308868,
      "learning_rate": 0.00018761662425784563,
      "loss": 2.4622,
      "step": 220
    },
    {
      "epoch": 0.2035839253525607,
      "grad_norm": 0.14331653714179993,
      "learning_rate": 0.00018648572236358498,
      "loss": 2.4575,
      "step": 240
    },
    {
      "epoch": 0.2205492524652741,
      "grad_norm": 0.1193879097700119,
      "learning_rate": 0.00018535482046932428,
      "loss": 2.4595,
      "step": 260
    },
    {
      "epoch": 0.2375145795779875,
      "grad_norm": 0.13083674013614655,
      "learning_rate": 0.00018422391857506363,
      "loss": 2.4441,
      "step": 280
    },
    {
      "epoch": 0.25447990669070086,
      "grad_norm": 0.1251763254404068,
      "learning_rate": 0.00018309301668080296,
      "loss": 2.4263,
      "step": 300
    },
    {
      "epoch": 0.2714452338034143,
      "grad_norm": 0.1331760287284851,
      "learning_rate": 0.00018196211478654228,
      "loss": 2.429,
      "step": 320
    },
    {
      "epoch": 0.28841056091612766,
      "grad_norm": 0.11522793024778366,
      "learning_rate": 0.0001808312128922816,
      "loss": 2.4193,
      "step": 340
    },
    {
      "epoch": 0.3053758880288411,
      "grad_norm": 0.14158713817596436,
      "learning_rate": 0.00017970031099802093,
      "loss": 2.3739,
      "step": 360
    },
    {
      "epoch": 0.32234121514155445,
      "grad_norm": 0.12232182174921036,
      "learning_rate": 0.00017856940910376026,
      "loss": 2.375,
      "step": 380
    },
    {
      "epoch": 0.3393065422542678,
      "grad_norm": 0.14064288139343262,
      "learning_rate": 0.00017743850720949958,
      "loss": 2.3968,
      "step": 400
    },
    {
      "epoch": 0.35627186936698124,
      "grad_norm": 0.14467568695545197,
      "learning_rate": 0.0001763076053152389,
      "loss": 2.3955,
      "step": 420
    },
    {
      "epoch": 0.3732371964796946,
      "grad_norm": 0.13606055080890656,
      "learning_rate": 0.00017517670342097824,
      "loss": 2.3685,
      "step": 440
    },
    {
      "epoch": 0.39020252359240803,
      "grad_norm": 0.14196783304214478,
      "learning_rate": 0.00017404580152671756,
      "loss": 2.3771,
      "step": 460
    },
    {
      "epoch": 0.4071678507051214,
      "grad_norm": 0.16323533654212952,
      "learning_rate": 0.0001729148996324569,
      "loss": 2.3546,
      "step": 480
    },
    {
      "epoch": 0.4241331778178348,
      "grad_norm": 0.1387929469347,
      "learning_rate": 0.0001717839977381962,
      "loss": 2.3091,
      "step": 500
    },
    {
      "epoch": 0.4410985049305482,
      "grad_norm": 0.17019854485988617,
      "learning_rate": 0.00017065309584393554,
      "loss": 2.3426,
      "step": 520
    },
    {
      "epoch": 0.45806383204326157,
      "grad_norm": 0.1579199582338333,
      "learning_rate": 0.0001695221939496749,
      "loss": 2.3221,
      "step": 540
    },
    {
      "epoch": 0.475029159155975,
      "grad_norm": 0.1543610692024231,
      "learning_rate": 0.0001683912920554142,
      "loss": 2.297,
      "step": 560
    },
    {
      "epoch": 0.49199448626868836,
      "grad_norm": 0.1496744155883789,
      "learning_rate": 0.00016726039016115354,
      "loss": 2.2647,
      "step": 580
    },
    {
      "epoch": 0.5089598133814017,
      "grad_norm": 0.15913057327270508,
      "learning_rate": 0.00016612948826689284,
      "loss": 2.2678,
      "step": 600
    },
    {
      "epoch": 0.5259251404941152,
      "grad_norm": 0.17261427640914917,
      "learning_rate": 0.0001649985863726322,
      "loss": 2.2531,
      "step": 620
    },
    {
      "epoch": 0.5428904676068286,
      "grad_norm": 0.14300137758255005,
      "learning_rate": 0.00016386768447837152,
      "loss": 2.2566,
      "step": 640
    },
    {
      "epoch": 0.5598557947195419,
      "grad_norm": 0.1589401364326477,
      "learning_rate": 0.00016273678258411082,
      "loss": 2.2592,
      "step": 660
    },
    {
      "epoch": 0.5768211218322553,
      "grad_norm": 0.16721752285957336,
      "learning_rate": 0.00016160588068985017,
      "loss": 2.2014,
      "step": 680
    },
    {
      "epoch": 0.5937864489449687,
      "grad_norm": 0.1744392216205597,
      "learning_rate": 0.00016047497879558947,
      "loss": 2.2042,
      "step": 700
    },
    {
      "epoch": 0.6107517760576822,
      "grad_norm": 0.36668145656585693,
      "learning_rate": 0.00015934407690132882,
      "loss": 2.1912,
      "step": 720
    },
    {
      "epoch": 0.6277171031703955,
      "grad_norm": 0.19349431991577148,
      "learning_rate": 0.00015821317500706815,
      "loss": 2.2112,
      "step": 740
    },
    {
      "epoch": 0.6446824302831089,
      "grad_norm": 0.18820586800575256,
      "learning_rate": 0.00015708227311280747,
      "loss": 2.2143,
      "step": 760
    },
    {
      "epoch": 0.6616477573958223,
      "grad_norm": 0.1821625530719757,
      "learning_rate": 0.0001559513712185468,
      "loss": 2.1713,
      "step": 780
    },
    {
      "epoch": 0.6786130845085356,
      "grad_norm": 0.16916915774345398,
      "learning_rate": 0.00015482046932428612,
      "loss": 2.1714,
      "step": 800
    },
    {
      "epoch": 0.6955784116212491,
      "grad_norm": 0.18424801528453827,
      "learning_rate": 0.00015368956743002545,
      "loss": 2.1552,
      "step": 820
    },
    {
      "epoch": 0.7125437387339625,
      "grad_norm": 0.20500966906547546,
      "learning_rate": 0.00015255866553576478,
      "loss": 2.1811,
      "step": 840
    },
    {
      "epoch": 0.7295090658466759,
      "grad_norm": 0.14913196861743927,
      "learning_rate": 0.0001514277636415041,
      "loss": 2.1443,
      "step": 860
    },
    {
      "epoch": 0.7464743929593892,
      "grad_norm": 0.19345901906490326,
      "learning_rate": 0.00015029686174724345,
      "loss": 2.1235,
      "step": 880
    },
    {
      "epoch": 0.7634397200721026,
      "grad_norm": 0.19165630638599396,
      "learning_rate": 0.00014916595985298275,
      "loss": 2.1466,
      "step": 900
    },
    {
      "epoch": 0.7804050471848161,
      "grad_norm": 0.16996507346630096,
      "learning_rate": 0.00014803505795872208,
      "loss": 2.1429,
      "step": 920
    },
    {
      "epoch": 0.7973703742975294,
      "grad_norm": 0.17593875527381897,
      "learning_rate": 0.0001469041560644614,
      "loss": 2.1139,
      "step": 940
    },
    {
      "epoch": 0.8143357014102428,
      "grad_norm": 0.1664142906665802,
      "learning_rate": 0.00014577325417020073,
      "loss": 2.1161,
      "step": 960
    },
    {
      "epoch": 0.8313010285229562,
      "grad_norm": 0.1673801690340042,
      "learning_rate": 0.00014464235227594008,
      "loss": 2.0723,
      "step": 980
    },
    {
      "epoch": 0.8482663556356697,
      "grad_norm": 0.2504444420337677,
      "learning_rate": 0.00014351145038167938,
      "loss": 2.1432,
      "step": 1000
    },
    {
      "epoch": 0.865231682748383,
      "grad_norm": 0.16862627863883972,
      "learning_rate": 0.00014238054848741873,
      "loss": 2.0775,
      "step": 1020
    },
    {
      "epoch": 0.8821970098610964,
      "grad_norm": 0.2221805900335312,
      "learning_rate": 0.00014124964659315806,
      "loss": 2.086,
      "step": 1040
    },
    {
      "epoch": 0.8991623369738098,
      "grad_norm": 0.24625609815120697,
      "learning_rate": 0.00014011874469889739,
      "loss": 2.0818,
      "step": 1060
    },
    {
      "epoch": 0.9161276640865231,
      "grad_norm": 0.24628117680549622,
      "learning_rate": 0.0001389878428046367,
      "loss": 2.0892,
      "step": 1080
    },
    {
      "epoch": 0.9330929911992366,
      "grad_norm": 0.19718319177627563,
      "learning_rate": 0.00013785694091037604,
      "loss": 2.0692,
      "step": 1100
    },
    {
      "epoch": 0.95005831831195,
      "grad_norm": 0.18782061338424683,
      "learning_rate": 0.00013672603901611536,
      "loss": 2.0967,
      "step": 1120
    },
    {
      "epoch": 0.9670236454246633,
      "grad_norm": 0.17796872556209564,
      "learning_rate": 0.0001355951371218547,
      "loss": 2.0772,
      "step": 1140
    },
    {
      "epoch": 0.9839889725373767,
      "grad_norm": 0.177534818649292,
      "learning_rate": 0.00013446423522759401,
      "loss": 2.0739,
      "step": 1160
    },
    {
      "epoch": 1.0008482663556357,
      "grad_norm": 0.17149576544761658,
      "learning_rate": 0.00013333333333333334,
      "loss": 2.0782,
      "step": 1180
    },
    {
      "epoch": 1.017813593468349,
      "grad_norm": 0.20156025886535645,
      "learning_rate": 0.00013220243143907267,
      "loss": 2.0715,
      "step": 1200
    },
    {
      "epoch": 1.0347789205810625,
      "grad_norm": 0.18010197579860687,
      "learning_rate": 0.000131071529544812,
      "loss": 2.0481,
      "step": 1220
    },
    {
      "epoch": 1.051744247693776,
      "grad_norm": 0.19784334301948547,
      "learning_rate": 0.00012994062765055132,
      "loss": 2.0599,
      "step": 1240
    },
    {
      "epoch": 1.0687095748064892,
      "grad_norm": 0.1853017359972,
      "learning_rate": 0.00012880972575629064,
      "loss": 2.0793,
      "step": 1260
    },
    {
      "epoch": 1.0856749019192027,
      "grad_norm": 0.2289772629737854,
      "learning_rate": 0.00012767882386203,
      "loss": 2.0525,
      "step": 1280
    },
    {
      "epoch": 1.102640229031916,
      "grad_norm": 0.208537757396698,
      "learning_rate": 0.0001265479219677693,
      "loss": 2.0395,
      "step": 1300
    },
    {
      "epoch": 1.1196055561446294,
      "grad_norm": 0.18681438267230988,
      "learning_rate": 0.00012541702007350865,
      "loss": 2.0318,
      "step": 1320
    },
    {
      "epoch": 1.136570883257343,
      "grad_norm": 0.16847491264343262,
      "learning_rate": 0.00012428611817924795,
      "loss": 2.05,
      "step": 1340
    },
    {
      "epoch": 1.1535362103700562,
      "grad_norm": 0.18833762407302856,
      "learning_rate": 0.00012315521628498727,
      "loss": 2.035,
      "step": 1360
    },
    {
      "epoch": 1.1705015374827696,
      "grad_norm": 0.17011940479278564,
      "learning_rate": 0.00012202431439072661,
      "loss": 2.0182,
      "step": 1380
    },
    {
      "epoch": 1.187466864595483,
      "grad_norm": 0.20378218591213226,
      "learning_rate": 0.00012089341249646594,
      "loss": 2.0506,
      "step": 1400
    },
    {
      "epoch": 1.2044321917081964,
      "grad_norm": 0.21199074387550354,
      "learning_rate": 0.00011976251060220527,
      "loss": 2.0281,
      "step": 1420
    },
    {
      "epoch": 1.2213975188209099,
      "grad_norm": 0.2147330790758133,
      "learning_rate": 0.00011863160870794459,
      "loss": 2.0309,
      "step": 1440
    },
    {
      "epoch": 1.2383628459336231,
      "grad_norm": 0.2172095775604248,
      "learning_rate": 0.00011750070681368393,
      "loss": 2.0443,
      "step": 1460
    },
    {
      "epoch": 1.2553281730463366,
      "grad_norm": 0.2247210443019867,
      "learning_rate": 0.00011636980491942324,
      "loss": 2.096,
      "step": 1480
    },
    {
      "epoch": 1.27229350015905,
      "grad_norm": 0.1832992285490036,
      "learning_rate": 0.00011523890302516258,
      "loss": 2.0328,
      "step": 1500
    },
    {
      "epoch": 1.2892588272717633,
      "grad_norm": 0.20238319039344788,
      "learning_rate": 0.0001141080011309019,
      "loss": 2.0052,
      "step": 1520
    },
    {
      "epoch": 1.3062241543844766,
      "grad_norm": 0.21793125569820404,
      "learning_rate": 0.00011297709923664124,
      "loss": 1.995,
      "step": 1540
    },
    {
      "epoch": 1.32318948149719,
      "grad_norm": 0.3068408668041229,
      "learning_rate": 0.00011184619734238055,
      "loss": 2.0088,
      "step": 1560
    },
    {
      "epoch": 1.3401548086099035,
      "grad_norm": 0.2080165147781372,
      "learning_rate": 0.00011071529544811987,
      "loss": 2.0198,
      "step": 1580
    },
    {
      "epoch": 1.3571201357226168,
      "grad_norm": 0.2905759811401367,
      "learning_rate": 0.0001095843935538592,
      "loss": 2.0134,
      "step": 1600
    },
    {
      "epoch": 1.3740854628353303,
      "grad_norm": 0.1914377361536026,
      "learning_rate": 0.00010845349165959853,
      "loss": 2.0299,
      "step": 1620
    },
    {
      "epoch": 1.3910507899480438,
      "grad_norm": 0.2027091532945633,
      "learning_rate": 0.00010732258976533787,
      "loss": 2.0075,
      "step": 1640
    },
    {
      "epoch": 1.408016117060757,
      "grad_norm": 0.2086980640888214,
      "learning_rate": 0.00010619168787107718,
      "loss": 2.0085,
      "step": 1660
    },
    {
      "epoch": 1.4249814441734705,
      "grad_norm": 0.26178696751594543,
      "learning_rate": 0.00010506078597681652,
      "loss": 2.0392,
      "step": 1680
    },
    {
      "epoch": 1.441946771286184,
      "grad_norm": 0.21913743019104004,
      "learning_rate": 0.00010392988408255583,
      "loss": 1.9927,
      "step": 1700
    },
    {
      "epoch": 1.4589120983988972,
      "grad_norm": 0.1783110350370407,
      "learning_rate": 0.00010279898218829517,
      "loss": 2.0505,
      "step": 1720
    },
    {
      "epoch": 1.4758774255116107,
      "grad_norm": 0.17192043364048004,
      "learning_rate": 0.0001016680802940345,
      "loss": 2.0374,
      "step": 1740
    },
    {
      "epoch": 1.492842752624324,
      "grad_norm": 0.19732089340686798,
      "learning_rate": 0.00010053717839977384,
      "loss": 2.0528,
      "step": 1760
    },
    {
      "epoch": 1.5098080797370375,
      "grad_norm": 0.18539050221443176,
      "learning_rate": 9.940627650551315e-05,
      "loss": 2.0203,
      "step": 1780
    },
    {
      "epoch": 1.5267734068497507,
      "grad_norm": 0.1962735652923584,
      "learning_rate": 9.827537461125248e-05,
      "loss": 2.0213,
      "step": 1800
    },
    {
      "epoch": 1.5437387339624642,
      "grad_norm": 0.19664214551448822,
      "learning_rate": 9.71444727169918e-05,
      "loss": 2.0075,
      "step": 1820
    },
    {
      "epoch": 1.5607040610751777,
      "grad_norm": 0.19679012894630432,
      "learning_rate": 9.601357082273114e-05,
      "loss": 2.0134,
      "step": 1840
    },
    {
      "epoch": 1.577669388187891,
      "grad_norm": 0.22056259214878082,
      "learning_rate": 9.488266892847047e-05,
      "loss": 1.9846,
      "step": 1860
    },
    {
      "epoch": 1.5946347153006044,
      "grad_norm": 0.20082241296768188,
      "learning_rate": 9.375176703420979e-05,
      "loss": 2.0213,
      "step": 1880
    },
    {
      "epoch": 1.611600042413318,
      "grad_norm": 0.20750512182712555,
      "learning_rate": 9.26208651399491e-05,
      "loss": 1.9755,
      "step": 1900
    },
    {
      "epoch": 1.6285653695260311,
      "grad_norm": 0.1959254890680313,
      "learning_rate": 9.148996324568843e-05,
      "loss": 2.0201,
      "step": 1920
    },
    {
      "epoch": 1.6455306966387444,
      "grad_norm": 0.1918560266494751,
      "learning_rate": 9.035906135142777e-05,
      "loss": 1.9853,
      "step": 1940
    },
    {
      "epoch": 1.662496023751458,
      "grad_norm": 0.20509418845176697,
      "learning_rate": 8.92281594571671e-05,
      "loss": 1.9672,
      "step": 1960
    },
    {
      "epoch": 1.6794613508641714,
      "grad_norm": 0.22786806523799896,
      "learning_rate": 8.809725756290642e-05,
      "loss": 2.0149,
      "step": 1980
    },
    {
      "epoch": 1.6964266779768846,
      "grad_norm": 0.21482308208942413,
      "learning_rate": 8.696635566864575e-05,
      "loss": 1.9758,
      "step": 2000
    },
    {
      "epoch": 1.713392005089598,
      "grad_norm": 0.1953495442867279,
      "learning_rate": 8.583545377438507e-05,
      "loss": 2.0025,
      "step": 2020
    },
    {
      "epoch": 1.7303573322023116,
      "grad_norm": 0.17900364100933075,
      "learning_rate": 8.470455188012441e-05,
      "loss": 1.9322,
      "step": 2040
    },
    {
      "epoch": 1.7473226593150248,
      "grad_norm": 0.2236165851354599,
      "learning_rate": 8.357364998586374e-05,
      "loss": 1.9758,
      "step": 2060
    },
    {
      "epoch": 1.7642879864277383,
      "grad_norm": 0.21169154345989227,
      "learning_rate": 8.244274809160306e-05,
      "loss": 1.9366,
      "step": 2080
    },
    {
      "epoch": 1.7812533135404518,
      "grad_norm": 0.18427373468875885,
      "learning_rate": 8.131184619734239e-05,
      "loss": 1.9819,
      "step": 2100
    },
    {
      "epoch": 1.798218640653165,
      "grad_norm": 0.2197924256324768,
      "learning_rate": 8.01809443030817e-05,
      "loss": 2.0107,
      "step": 2120
    },
    {
      "epoch": 1.8151839677658785,
      "grad_norm": 0.2037063091993332,
      "learning_rate": 7.905004240882104e-05,
      "loss": 1.9536,
      "step": 2140
    },
    {
      "epoch": 1.832149294878592,
      "grad_norm": 0.23212265968322754,
      "learning_rate": 7.791914051456037e-05,
      "loss": 1.9542,
      "step": 2160
    },
    {
      "epoch": 1.8491146219913053,
      "grad_norm": 0.28850623965263367,
      "learning_rate": 7.678823862029969e-05,
      "loss": 1.917,
      "step": 2180
    },
    {
      "epoch": 1.8660799491040185,
      "grad_norm": 0.2234908491373062,
      "learning_rate": 7.565733672603902e-05,
      "loss": 1.9963,
      "step": 2200
    },
    {
      "epoch": 1.883045276216732,
      "grad_norm": 0.18123768270015717,
      "learning_rate": 7.452643483177834e-05,
      "loss": 1.9533,
      "step": 2220
    },
    {
      "epoch": 1.9000106033294455,
      "grad_norm": 0.20844288170337677,
      "learning_rate": 7.339553293751767e-05,
      "loss": 1.9537,
      "step": 2240
    },
    {
      "epoch": 1.9169759304421587,
      "grad_norm": 0.19643814861774445,
      "learning_rate": 7.226463104325701e-05,
      "loss": 1.9924,
      "step": 2260
    },
    {
      "epoch": 1.9339412575548722,
      "grad_norm": 0.19987376034259796,
      "learning_rate": 7.113372914899633e-05,
      "loss": 1.9821,
      "step": 2280
    },
    {
      "epoch": 1.9509065846675857,
      "grad_norm": 0.20762096345424652,
      "learning_rate": 7.000282725473566e-05,
      "loss": 1.9848,
      "step": 2300
    },
    {
      "epoch": 1.967871911780299,
      "grad_norm": 0.20186878740787506,
      "learning_rate": 6.887192536047498e-05,
      "loss": 1.9816,
      "step": 2320
    },
    {
      "epoch": 1.9848372388930124,
      "grad_norm": 0.19120541214942932,
      "learning_rate": 6.774102346621431e-05,
      "loss": 1.98,
      "step": 2340
    },
    {
      "epoch": 2.0016965327112715,
      "grad_norm": 0.19516800343990326,
      "learning_rate": 6.661012157195364e-05,
      "loss": 1.994,
      "step": 2360
    },
    {
      "epoch": 2.0186618598239847,
      "grad_norm": 0.20522178709506989,
      "learning_rate": 6.547921967769296e-05,
      "loss": 1.9965,
      "step": 2380
    },
    {
      "epoch": 2.035627186936698,
      "grad_norm": 0.2161792516708374,
      "learning_rate": 6.434831778343229e-05,
      "loss": 1.9755,
      "step": 2400
    },
    {
      "epoch": 2.0525925140494117,
      "grad_norm": 0.23416341841220856,
      "learning_rate": 6.321741588917161e-05,
      "loss": 1.9679,
      "step": 2420
    },
    {
      "epoch": 2.069557841162125,
      "grad_norm": 0.18485018610954285,
      "learning_rate": 6.208651399491094e-05,
      "loss": 1.9796,
      "step": 2440
    },
    {
      "epoch": 2.086523168274838,
      "grad_norm": 0.18713556230068207,
      "learning_rate": 6.095561210065027e-05,
      "loss": 1.9817,
      "step": 2460
    },
    {
      "epoch": 2.103488495387552,
      "grad_norm": 0.23546366393566132,
      "learning_rate": 5.98247102063896e-05,
      "loss": 1.9657,
      "step": 2480
    },
    {
      "epoch": 2.120453822500265,
      "grad_norm": 0.19377796351909637,
      "learning_rate": 5.869380831212893e-05,
      "loss": 2.0001,
      "step": 2500
    },
    {
      "epoch": 2.1374191496129784,
      "grad_norm": 0.20381338894367218,
      "learning_rate": 5.7562906417868255e-05,
      "loss": 1.9568,
      "step": 2520
    },
    {
      "epoch": 2.1543844767256917,
      "grad_norm": 0.18354643881320953,
      "learning_rate": 5.643200452360759e-05,
      "loss": 1.9854,
      "step": 2540
    },
    {
      "epoch": 2.1713498038384054,
      "grad_norm": 0.18302486836910248,
      "learning_rate": 5.53011026293469e-05,
      "loss": 1.9677,
      "step": 2560
    },
    {
      "epoch": 2.1883151309511186,
      "grad_norm": 0.22881272435188293,
      "learning_rate": 5.417020073508623e-05,
      "loss": 1.978,
      "step": 2580
    },
    {
      "epoch": 2.205280458063832,
      "grad_norm": 0.22181740403175354,
      "learning_rate": 5.303929884082556e-05,
      "loss": 1.9449,
      "step": 2600
    },
    {
      "epoch": 2.2222457851765456,
      "grad_norm": 0.2174190729856491,
      "learning_rate": 5.1908396946564884e-05,
      "loss": 1.9611,
      "step": 2620
    },
    {
      "epoch": 2.239211112289259,
      "grad_norm": 0.2110297977924347,
      "learning_rate": 5.0777495052304216e-05,
      "loss": 1.9443,
      "step": 2640
    },
    {
      "epoch": 2.256176439401972,
      "grad_norm": 0.1987878382205963,
      "learning_rate": 4.964659315804354e-05,
      "loss": 1.9661,
      "step": 2660
    },
    {
      "epoch": 2.273141766514686,
      "grad_norm": 0.217937633395195,
      "learning_rate": 4.851569126378287e-05,
      "loss": 1.8962,
      "step": 2680
    },
    {
      "epoch": 2.290107093627399,
      "grad_norm": 0.2355271726846695,
      "learning_rate": 4.73847893695222e-05,
      "loss": 1.9307,
      "step": 2700
    },
    {
      "epoch": 2.3070724207401123,
      "grad_norm": 0.21693390607833862,
      "learning_rate": 4.6253887475261525e-05,
      "loss": 1.8872,
      "step": 2720
    },
    {
      "epoch": 2.3240377478528256,
      "grad_norm": 0.22059912979602814,
      "learning_rate": 4.512298558100085e-05,
      "loss": 1.9407,
      "step": 2740
    },
    {
      "epoch": 2.3410030749655393,
      "grad_norm": 0.2808436453342438,
      "learning_rate": 4.399208368674018e-05,
      "loss": 1.953,
      "step": 2760
    },
    {
      "epoch": 2.3579684020782525,
      "grad_norm": 0.2514861226081848,
      "learning_rate": 4.28611817924795e-05,
      "loss": 1.973,
      "step": 2780
    },
    {
      "epoch": 2.374933729190966,
      "grad_norm": 0.22106234729290009,
      "learning_rate": 4.1730279898218835e-05,
      "loss": 1.9558,
      "step": 2800
    },
    {
      "epoch": 2.3918990563036795,
      "grad_norm": 0.20054422318935394,
      "learning_rate": 4.059937800395816e-05,
      "loss": 1.9621,
      "step": 2820
    },
    {
      "epoch": 2.4088643834163928,
      "grad_norm": 0.2143537700176239,
      "learning_rate": 3.9468476109697486e-05,
      "loss": 2.0017,
      "step": 2840
    },
    {
      "epoch": 2.425829710529106,
      "grad_norm": 0.21303166449069977,
      "learning_rate": 3.833757421543681e-05,
      "loss": 1.981,
      "step": 2860
    },
    {
      "epoch": 2.4427950376418197,
      "grad_norm": 0.20942692458629608,
      "learning_rate": 3.720667232117614e-05,
      "loss": 1.9357,
      "step": 2880
    },
    {
      "epoch": 2.459760364754533,
      "grad_norm": 0.22312964498996735,
      "learning_rate": 3.607577042691547e-05,
      "loss": 1.9711,
      "step": 2900
    },
    {
      "epoch": 2.4767256918672462,
      "grad_norm": 0.1945742517709732,
      "learning_rate": 3.4944868532654796e-05,
      "loss": 1.9767,
      "step": 2920
    },
    {
      "epoch": 2.49369101897996,
      "grad_norm": 0.2147054374217987,
      "learning_rate": 3.381396663839412e-05,
      "loss": 1.9435,
      "step": 2940
    },
    {
      "epoch": 2.510656346092673,
      "grad_norm": 0.21366283297538757,
      "learning_rate": 3.268306474413345e-05,
      "loss": 1.9348,
      "step": 2960
    },
    {
      "epoch": 2.5276216732053864,
      "grad_norm": 0.24212320148944855,
      "learning_rate": 3.155216284987277e-05,
      "loss": 1.972,
      "step": 2980
    },
    {
      "epoch": 2.5445870003181,
      "grad_norm": 0.21521598100662231,
      "learning_rate": 3.0421260955612102e-05,
      "loss": 1.9337,
      "step": 3000
    },
    {
      "epoch": 2.5615523274308134,
      "grad_norm": 0.19441154599189758,
      "learning_rate": 2.929035906135143e-05,
      "loss": 1.953,
      "step": 3020
    },
    {
      "epoch": 2.5785176545435267,
      "grad_norm": 0.18737860023975372,
      "learning_rate": 2.8159457167090757e-05,
      "loss": 1.9884,
      "step": 3040
    },
    {
      "epoch": 2.59548298165624,
      "grad_norm": 0.1910795271396637,
      "learning_rate": 2.702855527283008e-05,
      "loss": 1.931,
      "step": 3060
    },
    {
      "epoch": 2.612448308768953,
      "grad_norm": 0.21351799368858337,
      "learning_rate": 2.5897653378569408e-05,
      "loss": 1.9742,
      "step": 3080
    },
    {
      "epoch": 2.629413635881667,
      "grad_norm": 0.2032293975353241,
      "learning_rate": 2.4766751484308737e-05,
      "loss": 1.9892,
      "step": 3100
    },
    {
      "epoch": 2.64637896299438,
      "grad_norm": 0.23954539000988007,
      "learning_rate": 2.3635849590048066e-05,
      "loss": 1.9545,
      "step": 3120
    },
    {
      "epoch": 2.6633442901070934,
      "grad_norm": 0.21722185611724854,
      "learning_rate": 2.2504947695787392e-05,
      "loss": 1.93,
      "step": 3140
    },
    {
      "epoch": 2.680309617219807,
      "grad_norm": 0.2086132913827896,
      "learning_rate": 2.1374045801526718e-05,
      "loss": 1.9556,
      "step": 3160
    },
    {
      "epoch": 2.6972749443325204,
      "grad_norm": 0.18439604341983795,
      "learning_rate": 2.0243143907266047e-05,
      "loss": 1.9473,
      "step": 3180
    },
    {
      "epoch": 2.7142402714452336,
      "grad_norm": 0.21600864827632904,
      "learning_rate": 1.9112242013005372e-05,
      "loss": 1.9764,
      "step": 3200
    },
    {
      "epoch": 2.7312055985579473,
      "grad_norm": 0.20329831540584564,
      "learning_rate": 1.7981340118744698e-05,
      "loss": 1.9579,
      "step": 3220
    },
    {
      "epoch": 2.7481709256706606,
      "grad_norm": 0.21187911927700043,
      "learning_rate": 1.6850438224484027e-05,
      "loss": 1.9456,
      "step": 3240
    },
    {
      "epoch": 2.765136252783374,
      "grad_norm": 0.21249815821647644,
      "learning_rate": 1.5719536330223353e-05,
      "loss": 1.9184,
      "step": 3260
    },
    {
      "epoch": 2.7821015798960875,
      "grad_norm": 0.20283424854278564,
      "learning_rate": 1.458863443596268e-05,
      "loss": 1.9674,
      "step": 3280
    },
    {
      "epoch": 2.799066907008801,
      "grad_norm": 0.2306373119354248,
      "learning_rate": 1.345773254170201e-05,
      "loss": 1.9587,
      "step": 3300
    },
    {
      "epoch": 2.816032234121514,
      "grad_norm": 0.25275105237960815,
      "learning_rate": 1.2326830647441335e-05,
      "loss": 1.943,
      "step": 3320
    },
    {
      "epoch": 2.8329975612342277,
      "grad_norm": 0.22607819736003876,
      "learning_rate": 1.1195928753180662e-05,
      "loss": 1.9541,
      "step": 3340
    },
    {
      "epoch": 2.849962888346941,
      "grad_norm": 0.18889766931533813,
      "learning_rate": 1.006502685891999e-05,
      "loss": 1.9669,
      "step": 3360
    },
    {
      "epoch": 2.8669282154596543,
      "grad_norm": 0.22780229151248932,
      "learning_rate": 8.934124964659315e-06,
      "loss": 1.9483,
      "step": 3380
    },
    {
      "epoch": 2.883893542572368,
      "grad_norm": 0.2017347663640976,
      "learning_rate": 7.803223070398644e-06,
      "loss": 2.0182,
      "step": 3400
    },
    {
      "epoch": 2.900858869685081,
      "grad_norm": 0.20989596843719482,
      "learning_rate": 6.67232117613797e-06,
      "loss": 1.9259,
      "step": 3420
    },
    {
      "epoch": 2.9178241967977945,
      "grad_norm": 0.23891346156597137,
      "learning_rate": 5.541419281877297e-06,
      "loss": 1.931,
      "step": 3440
    },
    {
      "epoch": 2.9347895239105077,
      "grad_norm": 0.388675332069397,
      "learning_rate": 4.410517387616624e-06,
      "loss": 1.9363,
      "step": 3460
    },
    {
      "epoch": 2.9517548510232214,
      "grad_norm": 0.24885933101177216,
      "learning_rate": 3.2796154933559517e-06,
      "loss": 1.9256,
      "step": 3480
    },
    {
      "epoch": 2.9687201781359347,
      "grad_norm": 0.20671682059764862,
      "learning_rate": 2.1487135990952787e-06,
      "loss": 1.9527,
      "step": 3500
    }
  ],
  "logging_steps": 20,
  "max_steps": 3537,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.9324028385755136e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
